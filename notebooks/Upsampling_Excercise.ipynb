{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello students! The following is the second task in the **Generative Computer Vision Models** module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below code block with simple autoencoder, introduce the upsampling layer according to the shape of the previous layer to reconstruct the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 25\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 14*14),\n",
    "            nn.ReLU()  # No sigmoid here, we'll use Mean Squared Error loss\n",
    "        )\n",
    "        # Todo : self.upsample (Upsampling layer)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(x.size(0), 1, 14, 14)  # Reshape to image size\n",
    "        # self.upsample(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and Train the autoencoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FashionMNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)  # Flatten the images\n",
    "        img = img.cuda() if torch.cuda.is_available() else img  # Move to GPU if available\n",
    "\n",
    "        # Forward pass\n",
    "        output = autoencoder(img)\n",
    "        output = output.view(img.size(0), -1)\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, img)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "\n",
    "# Save the trained model\n",
    "torch.save(autoencoder.state_dict(), 'model.pth') # change the name of the trained model according to the upsample layer used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "1. Design the upsampling layer using Nearest Neighbour Interpolation and train the above autoencoder & Plot the loss curves\n",
    "2. Design the upsampling layer using Bilinear Interpolation and train the above autoencoder & Plot the loss curves\n",
    "3. Design the upsampling layer using Bicubic Interpolation and train the above autoencoder & Plot the loss curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 \n",
    "1. Sample an image from the test set and evaluate the Fourier Transform of the input image and the reconstructed image using a trained autoencoder with different upsampling layers.\n",
    "2. Analyze the differences in the Fourier Transforms of the predictions obtained in the above task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
